# Bias–Variance Tradeoff: An Intuitive Demonstration

This repository explores the bias–variance tradeoff using a simple synthetic regression problem.

The goal is to build intuition by:
- generating multiple noisy datasets from a known true function,
- training many models on different datasets,
- visualizing their predictions,
- and empirically computing bias and variance.

The project was created as part of a learning process focused on understanding model behavior rather than optimizing performance.

## Key takeaway

Bias–variance decomposition explains why low-complexity models underfit
and high-complexity models overfit. An intermediate model complexity
achieves the best generalization by balancing bias and variance.
